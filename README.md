# Agentic Memory Foundation

A **lightweight, embedded RAG (Retrieval-Augmented Generation) library** built on Apache Lucene that provides the foundational infrastructure for building **agentic memory systems**. Store and search documents using vector embeddings with advanced chunking strategies—**no external server required**.

## What is Agentic Memory?

This library provides the core RAG infrastructure needed to build agentic memory systems—AI systems that autonomously organize, link, and evolve their knowledge over time. While the full vision of agentic memory includes LLM-driven note construction, autonomous link generation, and dynamic memory evolution, this library delivers the essential foundation: **flexible document storage, advanced chunking strategies, and powerful retrieval mechanisms**.

## Features

✅ **No External Server Required** - All data stored locally in Lucene index files  
✅ **Vector Similarity Search** - Using Lucene's KnnFloatVectorField for semantic search  
✅ **Keyword Search** - Traditional full-text search capabilities  
✅ **Hybrid Search** - Combine vector and keyword search with custom weights  
✅ **Metadata Support** - Attach custom metadata to documents  
✅ **Advanced Chunking Strategies** - 10+ extensible chunking strategies for optimal document segmentation  
✅ **Extensible Architecture** - Plugin your own chunking strategies and embedding providers  
✅ **Simple API** - Easy to use, clean interface  
✅ **Fully Tested** - Comprehensive unit tests included  

## Why "Agentic Memory Foundation"?

This library is positioned as a **foundation** for agentic memory because it provides:

1. **Flexible Knowledge Representation** - Multiple chunking strategies (sliding window, entity-based, topic-based, NER-based, etc.) allow adaptive knowledge organization
2. **Metadata-Rich Storage** - Documents can carry rich contextual metadata, enabling future link generation and relationship mapping
3. **Hybrid Retrieval** - Combines vector similarity with keyword search for context-aware retrieval
4. **Extensible Design** - ChunkingStrategy interface and EmbeddingProvider interface allow custom implementations

**What's Missing for Full Agentic Memory?**
- LLM-driven autonomous note construction
- Automatic link generation between related concepts
- Dynamic memory evolution and consolidation
- Graph-based context retrieval with relationship traversal




## Storage Architecture

This library uses **local file-based storage** via Apache Lucene:
- **No Qdrant server needed** - Unlike some vector databases, this runs entirely locally
- **No Docker containers** - No external dependencies to manage
- **File-based index** - Lucene creates and manages index files on your local filesystem
- **Embedded solution** - Everything runs in your Java application process



## Advanced Chunking Strategies

One of the key features that makes this library suitable as an agentic memory foundation is its **extensible chunking system**. Documents can be broken into chunks using various strategies before indexing, enabling more precise retrieval and knowledge organization.

### Available Chunking Strategies

| Strategy | Use Case | Example |
|----------|----------|---------|
| **SlidingWindowChunking** | General text processing | Research papers, articles |
| **AdaptiveChunking** | Section-based documents | Markdown, structured content |
| **EntityBasedChunking** | Entity-focused retrieval | Biographical data, company info |
| **TopicBasedChunking** | Topic segmentation | Multi-topic documents |
| **HybridChunking** | Complex multi-stage processing | Technical documentation |
| **TaskAwareChunking** | Task-specific chunking | Different chunking per use case |
| **HTMLTagBasedChunking** | Web content preservation | HTML documents, web scraping |
| **CodeSpecificChunking** | Source code files | Python, Java, etc. |
| **RegexChunking** | Pattern-based splitting | Log files, structured data |
| **NERBasedChunking** | Automatic entity detection | ML-powered entity extraction |

### Using Chunking Strategies

```java
import io.github.vishalmysore.*;
import io.github.vishalmysore.chunking.*;

// Create a chunking strategy
ChunkingStrategy strategy = new SlidingWindowChunking(150, 30); // 150 words, 30 overlap

// Add document with chunking
try (RAGService rag = new RAGService(indexPath, embeddings)) {
    rag.addDocumentWithChunking("doc1", longDocument, strategy);
    rag.commit();
    
    // Each chunk is indexed separately with ID: doc1_chunk_0, doc1_chunk_1, etc.
}
```

### Creating Custom Chunking Strategies

Implement the `ChunkingStrategy` interface:

```java
public class MyCustomChunking implements ChunkingStrategy {
    @Override
    public List<String> chunk(String content) {
        // Your custom logic here
        return Arrays.asList(content.split("CUSTOM_DELIMITER"));
    }
    
    @Override
    public String getName() {
        return "Custom Chunking";
    }
    
    @Override
    public String getDescription() {
        return "My custom chunking logic";
    }
}
```

See `examples/ChunkingStrategiesExample.java` and `CHUNKING_STRATEGIES.md` for detailed documentation.

## Quick Start

### 1. Basic Usage

```java
import io.github.vishalmysore.*;
import java.nio.file.Paths;

// Create a local index directory (stored on filesystem)
Path indexPath = Paths.get("my-rag-index");

// Initialize with embedding provider
EmbeddingProvider embeddings = new MockEmbeddingProvider(128);

try (RAGService rag = new RAGService(indexPath, embeddings)) {
    // Add documents
    rag.addDocument("doc1", "Machine learning is a subset of AI");
    rag.addDocument("doc2", "Python is a programming language");
    rag.commit();
    
    // Search semantically
    List<SearchResult> results = rag.search("artificial intelligence", 5);
    
    for (SearchResult result : results) {
        System.out.println(result.getContent() + " - Score: " + result.getScore());
    }
}
```

### 2. With Metadata

```java
Map<String, String> metadata = new HashMap<>();
metadata.put("author", "John Doe");
metadata.put("date", "2025-12-09");

rag.addDocument("doc1", "Content here", metadata);
rag.commit();

// Search returns metadata
List<SearchResult> results = rag.search("query", 10);
String author = results.get(0).getMetadata("author");
```

### 3. Retrieve Context for RAG

```java
// Get combined context from top-k documents
String context = rag.retrieveContext("What is machine learning?", 3);

// Use this context with your LLM
String prompt = "Context: " + context + "\n\nQuestion: What is machine learning?";
```

### 4. Load Previously Indexed Documents

The library persists all data to disk, so you can reload documents later:

```java
// Index documents in one session
try (RAGService rag = new RAGService(indexPath, embeddings)) {
    rag.addDocument("doc1", "Machine learning content");
    rag.addDocument("doc2", "AI content");
    rag.commit();
}

// Later, load the index in a new session
try (RAGService rag = new RAGService(indexPath, embeddings)) {
    // Get total document count
    int count = rag.getDocumentCount();
    System.out.println("Found " + count + " documents");
    
    // Retrieve a specific document by ID
    Document doc = rag.getDocumentById("doc1");
    if (doc != null) {
        System.out.println("Content: " + doc.getContent());
        System.out.println("Author: " + doc.getMetadata("author"));
    }
    
    // Retrieve all documents
    List<Document> allDocs = rag.getAllDocuments();
    for (Document d : allDocs) {
        System.out.println(d.getId() + ": " + d.getContent());
    }
    
    // Check if a document exists
    boolean exists = rag.documentExists("doc1");
    
    // Continue adding more documents to existing index
    rag.addDocument("doc3", "New content");
    rag.commit();
}
```

### 5. Low-Level API

```java
// Direct access to Lucene engine
LuceneRAGEngine engine = new LuceneRAGEngine(indexPath, 128);

// Create document with vector
Document doc = new Document.Builder()
    .id("doc1")
    .content("Text content")
    .vector(new float[]{0.1f, 0.2f, ...})
    .addMetadata("key", "value")
    .build();

engine.indexDocument(doc);
engine.commit();

// Vector search
float[] queryVector = {0.1f, 0.2f, ...};
List<SearchResult> results = engine.vectorSearch(queryVector, 10);

// Hybrid search (combines vector + keyword)
List<SearchResult> hybrid = engine.hybridSearch(
    queryVector, 
    "keyword query", 
    10,
    0.7f  // 70% vector, 30% keyword
);

engine.close();
```

## API Overview

### RAGService (High-Level API)

| Method | Description |
|--------|-------------|
| `addDocument(id, content)` | Add document with auto-generated embedding |
| `addDocument(id, content, metadata)` | Add document with metadata |
| `search(query, topK)` | Semantic search using embeddings |
| `keywordSearch(query, topK)` | Traditional keyword search |
| `hybridSearch(query, topK, weight)` | Combined vector + keyword search |
| `retrieveContext(query, topK)` | Get concatenated context for RAG |
| `getDocumentById(id)` | Retrieve a specific document by ID |
| `getAllDocuments()` | Retrieve all indexed documents |
| `documentExists(id)` | Check if a document exists |
| `getDocumentCount()` | Get total number of documents |
| `deleteDocument(id)` | Remove a document |
| `commit()` | Persist changes to disk |

### LuceneRAGEngine (Low-Level API)

| Method | Description |
|--------|-------------|
| `indexDocument(document)` | Index a document with vector |
| `indexDocuments(documents)` | Batch index multiple documents |
| `vectorSearch(vector, topK)` | KNN vector similarity search |
| `keywordSearch(text, topK)` | Full-text keyword search |
| `hybridSearch(...)` | Weighted combination search |
| `getDocumentById(id)` | Retrieve a document by ID |
| `getAllDocuments()` | Get all documents from index |
| `documentExists(id)` | Check document existence |
| `deleteDocument(id)` | Delete by ID |
| `getDocumentCount()` | Count indexed documents |

## Embedding Providers

The library supports multiple embedding providers:

### MockEmbeddingProvider (For Testing)

Simple hash-based embeddings for testing and development:

```java
EmbeddingProvider embeddings = new MockEmbeddingProvider(128);
```

### OpenAIEmbeddingProvider (Production)

Use OpenAI's embedding models for production:

```java
// With custom API URL
OpenAIEmbeddingProvider embeddings = new OpenAIEmbeddingProvider(
    "https://api.openai.com/v1/embeddings",  // API URL
    "your-api-key",                           // API Key
    "text-embedding-3-small",                 // Model
    1536                                      // Dimension
);

// Or use default OpenAI endpoint
OpenAIEmbeddingProvider embeddings = new OpenAIEmbeddingProvider(
    "your-api-key",
    "text-embedding-3-small",
    1536
);

// Supported models:
// - text-embedding-3-small (dimensions: 512, 1536)
// - text-embedding-3-large (dimensions: 256, 1024, 3072)
// - text-embedding-ada-002 (dimensions: 1536)

// Use with RAGService
try (RAGService rag = new RAGService(indexPath, embeddings)) {
    rag.addDocument("doc1", "Your content here");
    rag.commit();
}

// Don't forget to close when done
embeddings.close();
```

### Custom Embedding Providers

Implement the `EmbeddingProvider` interface for other models:

```java
public interface EmbeddingProvider {
    float[] embed(String text);
    int getDimension();
}
```

Examples:
- **Sentence Transformers** - Local models via ONNX or Python bridge
- **Azure OpenAI** - Use Azure's OpenAI service
- **Custom Models** - Any embedding model you prefer

## How It Works

1. **Indexing**: Documents are converted to vectors (embeddings) and stored in a local Lucene index
2. **Storage**: Lucene creates index files in the specified directory on your filesystem
3. **Search**: Query vectors are compared against stored vectors using cosine similarity (KNN)
4. **Results**: Top-K most similar documents are returned with scores

## Index Storage Location

The index is stored as **local files** in the directory you specify:

```java
Path indexPath = Paths.get("./lucene-index");  // Creates files in ./lucene-index/
```

You'll see files like:
- `segments_*`
- `*.fdx`, `*.fdt` (stored fields)
- `*.fnm` (field names)
- `*.nvd`, `*.nvm` (vector data)

**No server process runs** - the index is just files that Lucene reads/writes.

## Running Tests

```bash
mvn test
```

Tests cover:
- Document building and validation
- Vector search accuracy
- Metadata handling
- CRUD operations
- Edge cases and error handling

## Requirements

- Java 18 or higher
- Maven 3.6+
- No external services (Qdrant, Elasticsearch, etc.)

## Roadmap to Full Agentic Memory

This library currently provides the **foundational RAG infrastructure**. To evolve into a full agentic memory system, the following features are planned for the `io.github.vishalmysore.agenticmemory` package:

### Phase 1: Enhanced Knowledge Organization (Current)
- ✅ Multiple chunking strategies (10+ implementations)
- ✅ Extensible chunking architecture
- ✅ Metadata-rich document storage
- ✅ Hybrid search (vector + keyword)
- ✅ NER-based entity extraction

### Phase 2: Autonomous Note Construction (Planned)
- ⏳ LLM-driven note generation from raw content
- ⏳ Automatic summarization and condensation
- ⏳ Multi-document synthesis
- ⏳ Hierarchical note organization

### Phase 3: Link Generation & Knowledge Graph (Planned)
- ⏳ Automatic relationship detection between documents
- ⏳ Entity co-occurrence analysis
- ⏳ Temporal link tracking
- ⏳ Graph-based traversal API
- ⏳ Semantic link types (supports, contradicts, extends, etc.)

### Phase 4: Dynamic Memory Evolution (Planned)
- ⏳ Adaptive memory consolidation
- ⏳ Importance-based pruning
- ⏳ Temporal decay mechanisms
- ⏳ Knowledge update propagation
- ⏳ Conflict resolution strategies

### Phase 5: Context-Aware Retrieval (Planned)
- ⏳ Query expansion using knowledge graph
- ⏳ Multi-hop reasoning
- ⏳ Contextual relevance scoring
- ⏳ Conversation-aware retrieval
- ⏳ Personalized memory access

### Contributing to the Roadmap

We welcome contributions! If you're interested in helping build the agentic memory features:

1. **Chunking Strategies** - Add new chunking implementations (see `CustomChunkingExample.java`)
2. **Embedding Providers** - Integrate additional embedding models
3. **Memory Evolution** - Implement consolidation algorithms
4. **Link Generation** - Build entity relationship extractors
5. **Documentation** - Improve examples and guides

See `CONTRIBUTING.md` (coming soon) for guidelines.

## Why This Architecture?

### Current Strengths (RAG Foundation)
- **Local-first**: No external dependencies, pure file-based storage
- **Flexible**: Multiple chunking strategies for different content types
- **Extensible**: Plugin architecture for custom strategies and providers
- **Efficient**: Lucene's HNSW algorithm for fast vector search
- **Metadata-rich**: Supports arbitrary key-value annotations

### Design Decisions for Agentic Memory
- **Separation of Concerns**: Core RAG (`io.github.vishalmysore.rag`) vs. Agentic features (`io.github.vishalmysore.agenticmemory`)
- **Interface-driven**: `ChunkingStrategy` and `EmbeddingProvider` interfaces enable extensibility
- **Metadata Foundation**: Rich metadata support enables future link tracking and relationship mapping
- **Chunking Flexibility**: Different chunking strategies lay groundwork for task-aware knowledge organization

## Related Projects & Inspiration

- **A-Mem Paper** - "Agentic Memory: Autonomous Systems with Adaptive Recall" (inspiration for project vision)
- **LangChain** - General-purpose RAG framework (heavier, Python-focused)
- **LlamaIndex** - Document indexing framework (requires external services)
- **Qdrant** - Vector database server (separate process, network overhead)

**Our Difference**: Embedded, Java-native, extensible chunking, no external servers, agentic roadmap.

## License

This project is available under standard open source licenses.

## Example Output

```
Total documents indexed: 3

=== Semantic Search Results ===
1. [Score: 0.8523] Deep learning uses neural networks with multiple layers
   Author: Bob, Category: AI

2. [Score: 0.7891] Machine learning is a subset of artificial intelligence
   Author: Alice, Category: AI

✓ All data stored locally in: C:\work\navig\lucenerag\lucene-index
✓ No external server required!
```


